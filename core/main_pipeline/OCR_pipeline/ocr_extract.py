"""
ocr_extract.py
Module tr√≠ch xu·∫•t d·ªØ li·ªáu t·ª´ ·∫£nh - T√≠ch h·ª£p v·ªõi Vietnamese OCR Scanner
T·ªëi ∆∞u t·ªëc ƒë·ªô v·ªõi batch processing v√† cache
"""

import re
import json
import threading
from typing import List, Dict, Any, Optional
from pathlib import Path
import cv2
import numpy as np
# Import t·ª´ scanner module
try:
    from ocr_scan import VietnameseOCRService
    SCANNER_AVAILABLE = True
    print("‚úì Using optimized VietnameseOCRService from scanner")
except ImportError:
    SCANNER_AVAILABLE = False
    print("‚ö†Ô∏è  Scanner not available, using fallback PaddleOCR")
    from paddleocr import PaddleOCR

from postprocess_financial import PostProcessor


class OptimizedOCRExtractor:
    """
    Module tr√≠ch xu·∫•t OCR t·ªëi ∆∞u t·ªëc ƒë·ªô
    - S·ª≠ d·ª•ng VietnameseOCRService t·ª´ scanner (thread-safe)
    - Batch processing
    - Cache k·∫øt qu·∫£
    - Preprocessing ƒë∆∞·ª£c t·ªëi ∆∞u
    """
    
    def __init__(self, 
                 lang='vi', 
                 use_gpu=False, 
                 confidence_threshold=0.5,
                 use_scanner_service=True,
                 enable_cache=True):
        """
        Kh·ªüi t·∫°o OCR Extractor t·ªëi ∆∞u
        
        Args:
            lang: Ng√¥n ng·ªØ ('vi' cho ti·∫øng Vi·ªát)
            use_gpu: S·ª≠ d·ª•ng GPU hay kh√¥ng
            confidence_threshold: Ng∆∞·ª°ng ƒë·ªô tin c·∫≠y t·ªëi thi·ªÉu
            use_scanner_service: S·ª≠ d·ª•ng VietnameseOCRService (nhanh h∆°n)
            enable_cache: B·∫≠t cache k·∫øt qu·∫£ OCR
        """
        self.confidence_threshold = confidence_threshold
        self.enable_cache = enable_cache
        self.cache = {} if enable_cache else None
        self.cache_lock = threading.Lock() if enable_cache else None
        
        # Kh·ªüi t·∫°o OCR service
        if use_scanner_service and SCANNER_AVAILABLE:
            self.ocr_service = VietnameseOCRService()
            self.use_scanner = True
            print("‚úì Using optimized VietnameseOCRService (thread-safe)")
        else:
            self.ocr_service = None
            self.use_scanner = False
            self.ocr = PaddleOCR(
                use_angle_cls=True,
                lang=lang,
                use_gpu=use_gpu,
                show_log=False,
                det_db_thresh=0.3,
                det_db_box_thresh=0.6,
                rec_batch_num=6,
                drop_score=0.3,
                use_dilation=True,
            )
            print("‚úì Using standard PaddleOCR")
        
        self.post_processor = PostProcessor()
        print(f"‚úì Confidence threshold: {confidence_threshold}")
        print(f"‚úì Cache: {'Enabled' if enable_cache else 'Disabled'}")
    
    def preprocess_image(self, image: np.ndarray) -> np.ndarray:
        """
        Preprocessing ·∫£nh ƒë∆∞·ª£c t·ªëi ∆∞u t·ª´ scanner
        
        Args:
            image: ·∫¢nh ƒë·∫ßu v√†o (BGR)
            
        Returns:
            ·∫¢nh ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω
        """
        # Grayscale
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # Denoising
        denoised = cv2.fastNlMeansDenoising(
            gray, None, 
            h=10, 
            templateWindowSize=7, 
            searchWindowSize=21
        )
        
        # CLAHE (Contrast Limited Adaptive Histogram Equalization)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
        enhanced = clahe.apply(denoised)
        
        # Adaptive Thresholding
        binary = cv2.adaptiveThreshold(
            enhanced, 255, 
            cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
            cv2.THRESH_BINARY, 11, 2
        )
        
        # Morphological operations
        kernel = np.ones((1, 1), np.uint8)
        processed = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
        
        # Convert back to BGR
        processed = cv2.cvtColor(processed, cv2.COLOR_GRAY2BGR)
        
        return processed
    
    def get_cache_key(self, image_path: str) -> str:
        """T·∫°o cache key t·ª´ ƒë∆∞·ªùng d·∫´n ·∫£nh"""
        return str(Path(image_path).absolute())
    
    def get_from_cache(self, image_path: str) -> Optional[List[str]]:
        """L·∫•y k·∫øt qu·∫£ t·ª´ cache"""
        if not self.enable_cache:
            return None
        
        cache_key = self.get_cache_key(image_path)
        
        with self.cache_lock:
            return self.cache.get(cache_key)
    
    def save_to_cache(self, image_path: str, lines: List[str]):
        """L∆∞u k·∫øt qu·∫£ v√†o cache"""
        if not self.enable_cache:
            return
        
        cache_key = self.get_cache_key(image_path)
        
        with self.cache_lock:
            self.cache[cache_key] = lines
    
    def extract_text_from_image(self, image_path: str, use_preprocessing=True) -> List[str]:
        """
        Tr√≠ch xu·∫•t vƒÉn b·∫£n t·ª´ ·∫£nh (t·ªëi ∆∞u t·ªëc ƒë·ªô)
        
        Args:
            image_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file ·∫£nh
            use_preprocessing: S·ª≠ d·ª•ng preprocessing hay kh√¥ng
            
        Returns:
            Danh s√°ch c√°c d√≤ng vƒÉn b·∫£n ƒë√£ ƒë∆∞·ª£c s·∫Øp x·∫øp
        """
        # Ki·ªÉm tra cache tr∆∞·ªõc
        cached_result = self.get_from_cache(image_path)
        if cached_result is not None:
            print(f"  ‚úì Cache hit: {image_path}")
            return cached_result
        
        # Load image
        image = cv2.imread(image_path)
        if image is None:
            raise ValueError(f"Cannot load image: {image_path}")
        
        # Preprocessing (optional)
        if use_preprocessing:
            image = self.preprocess_image(image)
        
        # OCR
        if self.use_scanner:
            result = self.ocr_service.ocr(image, det=True, rec=True, cls=False)
        else:
            result = self.ocr.ocr(image, det=True, rec=True, cls=False)
        
        # Extract lines
        lines = []
        if result and result[0]:
            # S·∫Øp x·∫øp theo t·ªça ƒë·ªô y (t·ª´ tr√™n xu·ªëng d∆∞·ªõi)
            sorted_result = sorted(result[0], key=lambda x: x[0][0][1])
            
            for line in sorted_result:
                text = line[1][0]
                confidence = line[1][1]
                
                # Ch·ªâ l·∫•y c√°c d√≤ng c√≥ ƒë·ªô tin c·∫≠y cao
                if confidence > self.confidence_threshold:
                    lines.append(text)
        
        # Save to cache
        self.save_to_cache(image_path, lines)
        
        return lines
    
    def extract_from_scanner_result(self, scanner_result_path: str, page_number: int) -> List[str]:
        """
        Tr√≠ch xu·∫•t text t·ª´ k·∫øt qu·∫£ ƒë√£ scan (t√°i s·ª≠ d·ª•ng k·∫øt qu·∫£)
        
        Args:
            scanner_result_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file JSON k·∫øt qu·∫£ scanner
            page_number: S·ªë trang c·∫ßn tr√≠ch xu·∫•t
            
        Returns:
            Danh s√°ch c√°c d√≤ng vƒÉn b·∫£n
        """
        with open(scanner_result_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        page_key = f"page_{page_number}"
        
        if page_key not in data.get('pages', {}):
            raise ValueError(f"Page {page_number} not found in scanner result")
        
        page_data = data['pages'][page_key]
        content = page_data.get('content', '')
        
        # Parse content th√†nh lines
        lines = []
        
        # Extract t·ª´ [TEXT CONTENT]
        text_match = re.search(r'\[TEXT CONTENT\](.*?)\[/TEXT CONTENT\]', content, re.DOTALL)
        if text_match:
            text_content = text_match.group(1).strip()
            lines = [line.strip() for line in text_content.split('\n') if line.strip()]
        
        return lines
    
    def extract_dates_from_header(self, lines: List[str], max_lines_to_check=10) -> List[str]:
        """
        Tr√≠ch xu·∫•t c√°c ng√†y t·ª´ header c·ªßa b·∫£ng
        
        Args:
            lines: Danh s√°ch c√°c d√≤ng vƒÉn b·∫£n
            max_lines_to_check: S·ªë d√≤ng ƒë·∫ßu ti√™n c·∫ßn ki·ªÉm tra
            
        Returns:
            Danh s√°ch c√°c ng√†y th√°ng (t·ªëi ƒëa 2 ng√†y)
        """
        dates = []
        
        # T√¨m c√°c d√≤ng ch·ª©a ng√†y th√°ng (format dd/mm/yyyy)
        date_pattern = r'\d{2}/\d{2}/\d{4}'
        
        for line in lines[:max_lines_to_check]:
            found_dates = re.findall(date_pattern, line)
            dates.extend(found_dates)
        
        # N·∫øu kh√¥ng t√¨m th·∫•y, s·ª≠ d·ª•ng gi√° tr·ªã m·∫∑c ƒë·ªãnh
        if not dates:
            dates = ["31/03/2025", "01/01/2025"]
        
        return dates[:2]
    
    def batch_extract_text(self, image_paths: List[str], max_workers=4) -> Dict[str, List[str]]:
        """
        Tr√≠ch xu·∫•t text t·ª´ nhi·ªÅu ·∫£nh ƒë·ªìng th·ªùi (parallel processing)
        
        Args:
            image_paths: Danh s√°ch ƒë∆∞·ªùng d·∫´n ·∫£nh
            max_workers: S·ªë worker ƒë·ªìng th·ªùi
            
        Returns:
            Dict {image_path: lines}
        """
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        results = {}
        
        print(f"üì¶ Batch processing {len(image_paths)} images with {max_workers} workers...")
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Submit t·∫•t c·∫£ tasks
            future_to_path = {
                executor.submit(self.extract_text_from_image, path): path 
                for path in image_paths
            }
            
            # Collect results
            for i, future in enumerate(as_completed(future_to_path), 1):
                path = future_to_path[future]
                try:
                    lines = future.result()
                    results[path] = lines
                    print(f"  ‚úì [{i}/{len(image_paths)}] {Path(path).name}: {len(lines)} lines")
                except Exception as e:
                    print(f"  ‚úó [{i}/{len(image_paths)}] {Path(path).name}: {e}")
                    results[path] = []
        
        return results
    
    def process_and_export(self, 
                          image_path: str, 
                          output_json_path: str = None, 
                          verbose: bool = True,
                          use_preprocessing: bool = True) -> Dict[str, Any]:
        """
        X·ª≠ l√Ω ·∫£nh v√† xu·∫•t ra file JSON
        
        Args:
            image_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file ·∫£nh
            output_json_path: ƒê∆∞·ªùng d·∫´n file JSON ƒë·∫ßu ra (optional)
            verbose: Hi·ªÉn th·ªã log chi ti·∫øt
            use_preprocessing: S·ª≠ d·ª•ng preprocessing
            
        Returns:
            D·ªØ li·ªáu JSON ƒë√£ ƒë∆∞·ª£c c·∫•u tr√∫c
        """
        if verbose:
            print(f"üìÑ Processing: {image_path}")
        
        # B∆∞·ªõc 1: Tr√≠ch xu·∫•t vƒÉn b·∫£n t·ª´ ·∫£nh
        lines = self.extract_text_from_image(image_path, use_preprocessing)
        if verbose:
            print(f"  ‚úì Extracted {len(lines)} lines")
        
        # B∆∞·ªõc 2: Tr√≠ch xu·∫•t ng√†y th√°ng
        dates = self.extract_dates_from_header(lines)
        if verbose:
            print(f"  ‚úì Dates: {dates}")
        
        # B∆∞·ªõc 3: X·ª≠ l√Ω h·∫≠u k·ª≥ v√† x√¢y d·ª±ng c·∫•u tr√∫c
        structured_data = self.post_processor.build_structure(lines, dates)
        
        # B∆∞·ªõc 4: T·∫°o c·∫•u tr√∫c JSON cu·ªëi c√πng
        result = {
            'metadata': {
                'source_image': image_path,
                'dates': dates,
                'total_sections': len(structured_data),
                'total_lines': len(lines)
            },
            'sections': structured_data
        }
        
        # B∆∞·ªõc 5: L∆∞u file JSON (n·∫øu c√≥ ƒë∆∞·ªùng d·∫´n)
        if output_json_path:
            with open(output_json_path, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            if verbose:
                print(f"  ‚úì Saved to: {output_json_path}")
        
        return result
    
    def process_from_scanner_result(self,
                                   scanner_result_path: str,
                                   page_number: int,
                                   output_json_path: str = None) -> Dict[str, Any]:
        """
        X·ª≠ l√Ω t·ª´ k·∫øt qu·∫£ scanner c√≥ s·∫µn (t√°i s·ª≠ d·ª•ng, c·ª±c nhanh)
        
        Args:
            scanner_result_path: ƒê∆∞·ªùng d·∫´n file JSON k·∫øt qu·∫£ scanner
            page_number: S·ªë trang c·∫ßn x·ª≠ l√Ω
            output_json_path: ƒê∆∞·ªùng d·∫´n file JSON ƒë·∫ßu ra
            
        Returns:
            D·ªØ li·ªáu JSON ƒë√£ ƒë∆∞·ª£c c·∫•u tr√∫c
        """
        print(f"‚ôªÔ∏è  Reusing scanner result for page {page_number}")
        
        # B∆∞·ªõc 1: Tr√≠ch xu·∫•t text t·ª´ scanner result
        lines = self.extract_from_scanner_result(scanner_result_path, page_number)
        print(f"  ‚úì Extracted {len(lines)} lines from scanner result")
        
        # B∆∞·ªõc 2: Tr√≠ch xu·∫•t ng√†y th√°ng
        dates = self.extract_dates_from_header(lines)
        
        # B∆∞·ªõc 3: X·ª≠ l√Ω h·∫≠u k·ª≥
        structured_data = self.post_processor.build_structure(lines, dates)
        
        # B∆∞·ªõc 4: T·∫°o c·∫•u tr√∫c JSON
        result = {
            'metadata': {
                'source': 'scanner_result',
                'page_number': page_number,
                'dates': dates,
                'total_sections': len(structured_data),
                'total_lines': len(lines)
            },
            'sections': structured_data
        }
        
        # B∆∞·ªõc 5: L∆∞u file JSON
        if output_json_path:
            with open(output_json_path, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            print(f"  ‚úì Saved to: {output_json_path}")
        
        return result
    
    def batch_process_from_scanner(self,
                                  scanner_result_path: str,
                                  output_dir: str = None) -> List[Dict[str, Any]]:
        """
        X·ª≠ l√Ω batch t·ª´ k·∫øt qu·∫£ scanner (c·ª±c nhanh - kh√¥ng c·∫ßn OCR l·∫°i)
        
        Args:
            scanner_result_path: ƒê∆∞·ªùng d·∫´n file JSON k·∫øt qu·∫£ scanner
            output_dir: Th∆∞ m·ª•c l∆∞u c√°c file JSON
            
        Returns:
            Danh s√°ch k·∫øt qu·∫£ ƒë√£ x·ª≠ l√Ω
        """
        # Load scanner result
        with open(scanner_result_path, 'r', encoding='utf-8') as f:
            scanner_data = json.load(f)
        
        pages = scanner_data.get('pages', {})
        total_pages = len(pages)
        
        print(f"‚ôªÔ∏è  Batch processing {total_pages} pages from scanner result")
        
        results = []
        
        for i, page_key in enumerate(sorted(pages.keys()), 1):
            page_number = int(page_key.split('_')[1])
            
            output_path = None
            if output_dir:
                Path(output_dir).mkdir(parents=True, exist_ok=True)
                output_path = str(Path(output_dir) / f"page_{page_number}.json")
            
            try:
                result = self.process_from_scanner_result(
                    scanner_result_path, 
                    page_number, 
                    output_path
                )
                results.append(result)
                print(f"  ‚úì [{i}/{total_pages}] Page {page_number}")
            except Exception as e:
                print(f"  ‚úó [{i}/{total_pages}] Page {page_number}: {e}")
                results.append(None)
        
        return results
    
    def clear_cache(self):
        """X√≥a cache"""
        if self.enable_cache:
            with self.cache_lock:
                self.cache.clear()
                print("‚úì Cache cleared")


# V√≠ d·ª• s·ª≠ d·ª•ng
if __name__ == "__main__":
    print("="*70)
    print("OPTIMIZED OCR EXTRACTOR - EXAMPLES")
    print("="*70)
    
    # ===== C√ÅCH 1: Extract tr·ª±c ti·∫øp t·ª´ ·∫£nh (c√≥ cache) =====
    print("\n=== C√ÅCH 1: Extract t·ª´ ·∫£nh v·ªõi cache ===")
    extractor = OptimizedOCRExtractor(
        lang='vi',
        use_scanner_service=True,  # S·ª≠ d·ª•ng service t·ª´ scanner
        enable_cache=True           # B·∫≠t cache
    )
    
    result = extractor.process_and_export(
        image_path='balance_sheet.png',
        output_json_path='output.json'
    )
    
    print(f"\n‚úì Processed successfully")
    print(f"  Sections: {len(result['sections'])}")
    
    
    # ===== C√ÅCH 2: Batch processing nhi·ªÅu ·∫£nh =====
    print("\n\n=== C√ÅCH 2: Batch processing ===")
    
    image_files = [
        'image1.png',
        'image2.png',
        'image3.png'
    ]
    
    batch_results = extractor.batch_extract_text(
        image_paths=image_files,
        max_workers=4  # 4 workers ƒë·ªìng th·ªùi
    )
    
    print(f"\n‚úì Batch processed {len(batch_results)} images")
    
    
    # ===== C√ÅCH 3: T√°i s·ª≠ d·ª•ng k·∫øt qu·∫£ t·ª´ scanner (NHANH NH·∫§T) =====
    print("\n\n=== C√ÅCH 3: Reuse scanner result (FASTEST) ===")
    
    # Gi·∫£ s·ª≠ b·∫°n ƒë√£ ch·∫°y scanner tr∆∞·ªõc v√† c√≥ file ocr_results.json
    result_from_scanner = extractor.process_from_scanner_result(
        scanner_result_path='ocr_results.json',
        page_number=1,
        output_json_path='page_1_structured.json'
    )
    
    print(f"\n‚úì Reused scanner result")
    print(f"  Sections: {len(result_from_scanner['sections'])}")
    
    
    # ===== C√ÅCH 4: Batch processing t·ª´ scanner result =====
    print("\n\n=== C√ÅCH 4: Batch from scanner (ALL PAGES) ===")
    
    all_results = extractor.batch_process_from_scanner(
        scanner_result_path='ocr_results.json',
        output_dir='./structured_output'
    )
    
    print(f"\n‚úì Processed {len([r for r in all_results if r])} pages")
    
    
    # Clear cache
    print("\n\n=== Cache Management ===")
    extractor.clear_cache()